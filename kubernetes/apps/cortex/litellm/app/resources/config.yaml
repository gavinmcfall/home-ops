include:
  - model_config.yaml

general_settings:
  proxy_batch_write_at: 60
  database_connection_pool_limit: 10

  disable_spend_logs: false
  disable_error_logs: false

  background_health_checks: false
  health_check_interval: 300

  store_model_in_db: true

litellm_settings:
  proxy_server: true
  request_timeout: 600
  json_logs: true
  # enable_preview_features: true
  redact_user_api_key_info: true
  turn_off_message_logging: false
  set_verbose: true

  # Caching settings
  cache: true
  cache_params:
    type: qdrant-semantic        # redis, s3, redis-semantic, qdrant-semantic, disk, () not specificed = In memory

    # https://docs.litellm.ai/docs/caching/all_caches#updating-cache-params-redis-host-port-etc

    # Optional - Qdrant Semantic Cache Settings
    qdrant_semantic_cache_embedding_model: text-embedding-3-small # the model should be defined on the model_list
    qdrant_collection_name: litellm_cache
    qdrant_quantization_config: binary
    similarity_threshold: 0.8   # similarity threshold for semantic cache

    # Common Cache settings
    # Optional - Supported call types for caching

    supported_call_types: ["acompletion", "atext_completion", "aembedding", "atranscription"]
                          # /chat/completions, /completions, /embeddings, /audio/transcriptions
    mode: default_on # if default_off, you need to opt in to caching on a per call basis
    ttl: 600 # ttl for caching



router:
  - destination: vision_openai
    if:
      contains:
        - "data:image"
        - "image analysis"
        - "describe this image"
        - "photo"
        - "diagram"

router_settings:
  fallbacks:
    vision_openai:
      - vision_anthropic
      - vision_gemini
      - vision_xai
      - vision_bedrock
    general_openai:
      - general_anthropic
      - general_gemini
      - general_xai
    code_fallback_together:
      - code_fallback_groq
      - vision_openai  # For code+diagram scenarios
    code_fallback_groq:
      - code_fallback_together
      - general_openai
